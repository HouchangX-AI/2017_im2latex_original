[Trainer.py], 2020-02-24 11:44:22: Batch 0: loss=6.645383834838867, acc=0.013157894736842105, lr=0.001
[Trainer.py], 2020-02-24 11:48:34: start training one epoch
[Trainer.py], 2020-02-24 11:51:04: Batch 0: loss=6.646030902862549, acc=0.013157894736842105, lr=0.001
[Trainer.py], 2020-02-24 12:03:17: Batch 5: loss=6.498889923095703, acc=0.02127659574468085, lr=0.001
[Trainer.py], 2020-02-24 12:16:17: Batch 10: loss=6.749333381652832, acc=0.14634146341463414, lr=0.001
[Trainer.py], 2020-02-24 12:28:44: Batch 15: loss=4.770725250244141, acc=0.3611111111111111, lr=0.001
[Trainer.py], 2020-02-24 12:41:55: Batch 20: loss=5.484915733337402, acc=0.34285714285714286, lr=0.001
[Trainer.py], 2020-02-24 12:54:34: Batch 25: loss=4.194648742675781, acc=0.45454545454545453, lr=0.001
[Trainer.py], 2020-02-24 13:09:20: Batch 30: loss=6.1136884689331055, acc=0.1875, lr=0.001
[Trainer.py], 2020-02-24 13:22:36: Batch 35: loss=5.218989849090576, acc=0.3, lr=0.001
[Trainer.py], 2020-02-24 13:35:38: Batch 40: loss=5.695699214935303, acc=0.13793103448275862, lr=0.001
[Trainer.py], 2020-02-24 13:48:40: Batch 45: loss=5.803713321685791, acc=0.14285714285714285, lr=0.001
[Trainer.py], 2020-02-24 14:01:20: Batch 50: loss=4.0592193603515625, acc=0.4444444444444444, lr=0.001
[Trainer.py], 2020-02-25 05:44:34: start training one epoch
[Trainer.py], 2020-02-25 05:48:44: Batch 0: loss=6.6460113525390625, acc=0.015037593984962405, lr=0.001
[Trainer.py], 2020-02-25 06:30:31: Batch 10: loss=4.199076175689697, acc=0.4492753623188406, lr=0.001
[Trainer.py], 2020-02-25 07:10:17: Batch 20: loss=4.909434795379639, acc=0.27586206896551724, lr=0.001
[Trainer.py], 2020-02-25 07:50:28: Batch 30: loss=3.8023128509521484, acc=0.46, lr=0.001
[Trainer.py], 2020-02-25 08:30:46: Batch 40: loss=5.27102518081665, acc=0.34782608695652173, lr=0.001
[Trainer.py], 2020-02-25 09:10:01: Batch 50: loss=5.334321022033691, acc=0.35714285714285715, lr=0.001
[Trainer.py], 2020-02-25 09:51:39: Batch 60: loss=4.158618450164795, acc=0.46153846153846156, lr=0.001
[Trainer.py], 2020-02-25 10:33:32: Batch 70: loss=4.9177446365356445, acc=0.3611111111111111, lr=0.001
[Trainer.py], 2020-02-25 11:17:40: Batch 80: loss=3.6598901748657227, acc=0.4166666666666667, lr=0.001
[Trainer.py], 2020-02-25 11:59:20: Batch 90: loss=3.1882519721984863, acc=0.5882352941176471, lr=0.001
[Trainer.py], 2020-02-25 12:40:48: Batch 100: loss=3.4267008304595947, acc=0.5, lr=0.001
[Trainer.py], 2020-02-25 13:22:53: Batch 110: loss=4.081568241119385, acc=0.53125, lr=0.001
[Trainer.py], 2020-02-25 14:02:29: Batch 120: loss=2.940897226333618, acc=0.5666666666666667, lr=0.001
[Trainer.py], 2020-02-25 17:03:36: Batch 130: loss=3.2046611309051514, acc=0.6071428571428571, lr=0.001
[Trainer.py], 2020-02-25 17:42:10: Batch 140: loss=3.0615110397338867, acc=0.5, lr=0.001
[Trainer.py], 2020-02-25 18:22:25: Batch 150: loss=3.916452169418335, acc=0.38461538461538464, lr=0.001
[Trainer.py], 2020-02-26 05:29:25: Batch 160: loss=2.011829137802124, acc=0.6538461538461539, lr=0.001
[Trainer.py], 2020-02-26 06:12:56: Batch 170: loss=2.714411735534668, acc=0.625, lr=0.001
[Trainer.py], 2020-02-26 06:55:42: Batch 180: loss=3.1557188034057617, acc=0.5833333333333334, lr=0.001
[Trainer.py], 2020-02-26 07:37:24: Batch 190: loss=3.260521650314331, acc=0.4583333333333333, lr=0.001
[Trainer.py], 2020-02-26 08:41:36: Batch 200: loss=4.310639381408691, acc=0.4090909090909091, lr=0.001
[Trainer.py], 2020-02-26 09:19:18: Batch 210: loss=2.941702365875244, acc=0.6363636363636364, lr=0.001
[Trainer.py], 2020-02-26 10:03:27: Batch 220: loss=2.4932165145874023, acc=0.7727272727272727, lr=0.001
[Trainer.py], 2020-02-26 10:48:05: Batch 230: loss=2.4826483726501465, acc=0.6, lr=0.001
[Trainer.py], 2020-02-26 11:49:40: Batch 240: loss=4.1465163230896, acc=0.45, lr=0.001
[Trainer.py], 2020-02-26 12:39:10: Batch 250: loss=1.343576431274414, acc=0.75, lr=0.001
[Trainer.py], 2020-02-26 13:22:20: Batch 260: loss=2.3165242671966553, acc=0.6842105263157895, lr=0.001
[Trainer.py], 2020-02-26 14:06:40: Batch 270: loss=1.8029985427856445, acc=0.8333333333333334, lr=0.001
[Trainer.py], 2020-02-26 16:25:42: Batch 280: loss=1.9761987924575806, acc=0.8333333333333334, lr=0.001
[Trainer.py], 2020-02-26 17:02:44: Batch 290: loss=1.0685206651687622, acc=0.8888888888888888, lr=0.001
[Trainer.py], 2020-02-26 17:40:07: Batch 300: loss=0.2977431118488312, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 05:19:29: Batch 310: loss=3.1642606258392334, acc=0.5625, lr=0.001
[Trainer.py], 2020-02-27 06:01:43: Batch 320: loss=1.2159063816070557, acc=0.875, lr=0.001
[Trainer.py], 2020-02-27 06:44:58: Batch 330: loss=2.5946691036224365, acc=0.8125, lr=0.001
[Trainer.py], 2020-02-27 07:25:26: Batch 340: loss=1.5003776550292969, acc=0.7857142857142857, lr=0.001
[Trainer.py], 2020-02-27 08:06:40: Batch 350: loss=0.5937979817390442, acc=0.9285714285714286, lr=0.001
[Trainer.py], 2020-02-27 08:48:06: Batch 360: loss=1.3290023803710938, acc=0.7857142857142857, lr=0.001
[Trainer.py], 2020-02-27 09:26:47: Batch 370: loss=1.2101819515228271, acc=0.8571428571428571, lr=0.001
[Trainer.py], 2020-02-27 10:09:27: Batch 380: loss=2.4778990745544434, acc=0.5833333333333334, lr=0.001
[Trainer.py], 2020-02-27 10:50:31: Batch 390: loss=0.9272085428237915, acc=0.8333333333333334, lr=0.001
[Trainer.py], 2020-02-27 11:36:29: Batch 400: loss=0.0966302827000618, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 12:22:08: Batch 410: loss=2.643805742263794, acc=0.7272727272727273, lr=0.001
[Trainer.py], 2020-02-27 13:05:19: Batch 420: loss=2.146054983139038, acc=0.8, lr=0.001
[Trainer.py], 2020-02-27 13:49:41: Batch 430: loss=0.05609779804944992, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 15:55:40: Batch 440: loss=0.20262786746025085, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 16:40:42: Batch 450: loss=0.6441341042518616, acc=0.75, lr=0.001
[Trainer.py], 2020-02-27 17:54:02: Batch 460: loss=0.08729139715433121, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 19:23:50: Batch 470: loss=0.0834565982222557, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 20:41:14: Batch 480: loss=0.031745199114084244, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 22:01:48: Batch 490: loss=0.0028302879072725773, acc=1.0, lr=0.001
[Trainer.py], 2020-02-27 23:16:03: Epoch finished, loss=2.4363838422199477 acc=0.6759724268388173, lr=0.001
[Trainer.py], 2020-02-27 23:16:03: training one epoch finished.
[Trainer.py], 2020-02-27 23:16:04: Epoch 1 saved.
